{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import pickle\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "\n",
    "pd.set_option('display.max_columns', None) # no truncate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "data_path_out = 'Data/output/'\n",
    "    \n",
    "# Deserialize previously saved data from \"data-visualization\"\n",
    "df = {}\n",
    "for df_name in ['train', 'test']:\n",
    "    with open(data_path_out+df_name+'_dv.obj', 'rb') as file:\n",
    "        df[df_name] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['train'].copy()\n",
    "X = X.drop(columns=['NumberOfCustomers', 'NumberOfSales'], axis=1)\n",
    "y = df['train'].loc[:, 'NumberOfCustomers':'NumberOfSales']\n",
    "\n",
    "all_data = pd.concat([X, df['test']], axis=0).reset_index()\n",
    "\n",
    "#to restore the original column order\n",
    "all_data = all_data[list(df['test'].columns.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_Gust_SpeedKm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop('Max_Gust_SpeedKm_h', axis = 1)\n",
    "df['train'] = df['train'].drop('Max_Gust_SpeedKm_h', axis = 1)\n",
    "df['test'] = df['test'].drop('Max_Gust_SpeedKm_h', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values of Events left\n"
     ]
    }
   ],
   "source": [
    "all_data.Events = all_data.Events.fillna('Normal')\n",
    "print (\"There are {} missing values of Events left\".format(all_data['Events'].isna().sum(),\n",
    "                                                                  all_data['Events'].isna().sum()/\n",
    "                                                                  all_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Visibility has a high negative correlation  with Humidity, a good imputation method could be to group data by Humidity and use the median value to fill in missing values.\n",
    "\n",
    "Intuitively the minimum visibility corresponds to the maximum humidity and viceversa.\n",
    "\n",
    "Some humidity levels could not have visiblity values, so a good solution could be to take the mean between the two adiacent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaprimo/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1018: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "100%|██████████| 13959/13959 [00:26<00:00, 530.76it/s]\n",
      "100%|██████████| 13959/13959 [00:29<00:00, 476.73it/s]\n",
      "100%|██████████| 13959/13959 [00:27<00:00, 514.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values of Min_VisibilitykM left\n",
      "There are 0 missing values of Max_VisibilityKm left\n",
      "There are 0 missing values of Mean_VisibilityKm left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_vis_dict=all_data.groupby('Max_Humidity')['Min_VisibilitykM'].apply(lambda x: x.median()).to_dict()\n",
    "max_vis_dict=all_data.groupby('Min_Humidity')['Max_VisibilityKm'].apply(lambda x: x.median()).to_dict()\n",
    "mean_vis_dict=all_data.groupby('Mean_Humidity')['Mean_VisibilityKm'].apply(lambda x: x.median()).to_dict()\n",
    "\n",
    "#Function that replaces missing values of the median with the mean of next and previous values\n",
    "def replace_nan_median(visdict_list):\n",
    "    for visdict in visdict_list:\n",
    "        item_list=[visdict[x] for x in visdict.keys()]\n",
    "        for val in range(len(item_list)):\n",
    "            if(math.isnan(item_list[val])):\n",
    "                prev_val = (val-1) < 0 and item_list[val+1]  or item_list[val-1]\n",
    "                next_val = (val+1) > len(item_list) and item_list[val-1] or item_list[val+1]\n",
    "                item_list[val] = float(int((prev_val + next_val)/2))\n",
    "        iterator = 0\n",
    "        for k,i in visdict.items():\n",
    "            visdict[k]=item_list[iterator]\n",
    "            iterator+=1\n",
    "\n",
    "#Function that replaces nan in the dataframe with the dictionary\n",
    "#value corresponding to the samples humidity level\n",
    "\n",
    "def replace_nan_df(col_name1,col_name2,dictionary):\n",
    "    nan_index=all_data[all_data[col_name1].isnull()].index.tolist()\n",
    "    for index in tqdm(nan_index):\n",
    "        humidty_lvl = all_data.loc[index,col_name2] \n",
    "        all_data.loc[index,col_name1] = dictionary[humidty_lvl]\n",
    "\n",
    "    \n",
    "        \n",
    "replace_nan_median([min_vis_dict,max_vis_dict,mean_vis_dict])\n",
    "\n",
    "#Replacing nan in dataframe \n",
    "replace_nan_df('Min_VisibilitykM','Max_Humidity',min_vis_dict)\n",
    "replace_nan_df('Max_VisibilityKm','Min_Humidity',max_vis_dict)\n",
    "replace_nan_df('Mean_VisibilityKm','Mean_Humidity',mean_vis_dict)\n",
    "\n",
    "for col in [\"Min_VisibilitykM\",\"Max_VisibilityKm\",\"Mean_VisibilityKm\"]:\n",
    "    print (\"There are {} missing values of {} left\".\n",
    "           format(all_data[col].isna().sum(),col))\n",
    "os.system('say \"Visibility Done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Cover [ERROR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute with regression below\n",
    "all_data['CloudCover'] = all_data['CloudCover'].fillna(all_data['CloudCover'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568710, 69)\n",
      "(0, 69)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by RobustScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ea301d136b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mX_train_cloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_cloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# kf = KFold(5, shuffle=True, random_state=42).get_n_splits(X_train_cloud)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_check_array\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;34m\"\"\"Makes sure centering is not enabled for sparse matrices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m-> 1026\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 462\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by RobustScaler."
     ]
    }
   ],
   "source": [
    "rf =  RandomForestClassifier(n_estimators=50,verbose=True)\n",
    "\n",
    "cloud_data = all_data.copy()\n",
    "cloud_data = pd.get_dummies(cloud_data)\n",
    "cloud_data['Date_delta'] = (cloud_data['Date'] - cloud_data['Date'].min())  / np.timedelta64(1,'D')\n",
    "cloud_data=cloud_data.drop('Date',axis=1)\n",
    "nan_indexes=cloud_data[cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "good_indexes =cloud_data[~cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "\n",
    "train_cloud = cloud_data.loc[good_indexes]\n",
    "test_cloud = cloud_data.loc[nan_indexes]\n",
    "print(train_cloud.shape)\n",
    "print(test_cloud.shape)\n",
    "\n",
    "X_train_cloud = train_cloud.copy()\n",
    "X_train_cloud = X_train_cloud.drop(['CloudCover'], axis=1)\n",
    "y_train_cloud = train_cloud['CloudCover']\n",
    "test_cloud = test_cloud.drop('CloudCover',axis=1)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_cloud = scaler.fit_transform(X_train_cloud)\n",
    "test_cloud = scaler.transform(test_cloud)\n",
    "\n",
    "# kf = KFold(5, shuffle=True, random_state=42).get_n_splits(X_train_cloud)\n",
    "# score_CV= cross_val_score(rf, X_train_cloud, y_train_cloud, cv = kf)\n",
    "\n",
    "# print(\"The 10-fold crossvalidation of RF is {:.5f} +/- {:.3f}\".format(score_CV.mean(),score_CV.std()))\n",
    "# os.system('say \"Crossvalidation done.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf.fit(X_train_cloud,y_train_cloud)\n",
    "# pred_results =rf.predict(test_cloud)\n",
    "nan_index=cloud_data[cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "result_df = pd.DataFrame(data={' Index' : nan_index,\n",
    "                               'CloudCover': pred_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region 2 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
