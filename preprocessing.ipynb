{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import skew, norm\n",
    "\n",
    "pd.set_option('display.max_columns', None) # no truncate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "data_path_out = 'Data/output/'\n",
    "    \n",
    "# Deserialize previously saved data from \"data-visualization\"\n",
    "df = {}\n",
    "for df_name in ['train', 'test', 'all', 'monthly']:\n",
    "    with open(data_path_out+df_name+'_dv.obj', 'rb') as file:\n",
    "        df[df_name] = pickle.load(file)\n",
    "        \n",
    "all_data = df['all']\n",
    "train = df['train']\n",
    "test = df['test']\n",
    "monthly_sales = df['monthly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_Gust_SpeedKm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop('Max_Gust_SpeedKm_h', axis=1)\n",
    "df['train'] = df['train'].drop('Max_Gust_SpeedKm_h', axis=1)\n",
    "df['test'] = df['test'].drop('Max_Gust_SpeedKm_h', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumberOfCustomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in ['train', 'monthly']:\n",
    "    df[df_name] = df[df_name].drop('NumberOfCustomers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values of Events left\n"
     ]
    }
   ],
   "source": [
    "all_data.Events = all_data.Events.fillna('Normal')\n",
    "print ('There are {} missing values of Events left'\\\n",
    "       .format(all_data['Events'].isna().sum(),\n",
    "               all_data['Events'].isna().sum()/all_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Visibility has a high negative correlation  with Humidity, a good imputation method could be to group data by Humidity and use the median value to fill in missing values.\n",
    "\n",
    "Intuitively the minimum visibility corresponds to the maximum humidity and viceversa.\n",
    "\n",
    "Some humidity levels could not have visiblity values, so a good solution could be to take the mean between the two adiacent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that replaces missing values of the median with the mean of next and previous values\n",
    "def replace_nan_median(visdict_list):\n",
    "    for visdict in visdict_list:\n",
    "        item_list=[visdict[x] for x in visdict.keys()]\n",
    "        for val in range(len(item_list)):\n",
    "            if(math.isnan(item_list[val])):\n",
    "                prev_val = (val-1) < 0 and item_list[val+1]  or item_list[val-1]\n",
    "                next_val = (val+1) > len(item_list) and item_list[val-1] or item_list[val+1]\n",
    "                item_list[val] = float(int((prev_val+next_val)/2))\n",
    "        iterator = 0\n",
    "        for k,i in visdict.items():\n",
    "            visdict[k] = item_list[iterator]\n",
    "            iterator += 1\n",
    "\n",
    "# Function that replaces nan in the dataframe with the dictionary\n",
    "# value corresponding to the samples humidity level\n",
    "def replace_nan_df(col_name1,col_name2,dictionary):\n",
    "    nan_index=all_data[all_data[col_name1].isnull()].index.tolist()\n",
    "    for index in tqdm(nan_index):\n",
    "        humidty_lvl = all_data.loc[index,col_name2] \n",
    "        all_data.loc[index,col_name1] = dictionary[humidty_lvl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaprimo/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1018: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "100%|██████████| 13959/13959 [00:27<00:00, 509.45it/s]\n",
      "100%|██████████| 13959/13959 [00:27<00:00, 509.47it/s]\n",
      "100%|██████████| 13959/13959 [00:27<00:00, 510.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values of Min_VisibilitykM left\n",
      "There are 0 missing values of Max_VisibilityKm left\n",
      "There are 0 missing values of Mean_VisibilityKm left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_vis_dict = all_data.groupby('Max_Humidity')['Min_VisibilitykM'].apply(lambda x: x.median()).to_dict()\n",
    "max_vis_dict = all_data.groupby('Min_Humidity')['Max_VisibilityKm'].apply(lambda x: x.median()).to_dict()\n",
    "mean_vis_dict = all_data.groupby('Mean_Humidity')['Mean_VisibilityKm'].apply(lambda x: x.median()).to_dict()\n",
    "        \n",
    "replace_nan_median([min_vis_dict,max_vis_dict,mean_vis_dict])\n",
    "\n",
    "#Replacing nan in dataframe \n",
    "replace_nan_df('Min_VisibilitykM', 'Max_Humidity', min_vis_dict)\n",
    "replace_nan_df('Max_VisibilityKm', 'Min_Humidity', max_vis_dict)\n",
    "replace_nan_df('Mean_VisibilityKm', 'Mean_Humidity', mean_vis_dict)\n",
    "\n",
    "for col in [\"Min_VisibilitykM\", \"Max_VisibilityKm\", \"Mean_VisibilityKm\"]:\n",
    "    print (\"There are {} missing values of {} left\".\n",
    "           format(all_data[col].isna().sum(),col))\n",
    "os.system('say \"Visibility Done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519111, 69)\n",
      "(49599, 69)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, verbose=True)\n",
    "\n",
    "# enable or not cross validation\n",
    "cv_mode = False\n",
    "\n",
    "cloud_data = all_data.copy()\n",
    "cloud_data = pd.get_dummies(cloud_data)\n",
    "cloud_data['Date_delta'] = (cloud_data['Date'] - cloud_data['Date'].min()) / np.timedelta64(1, 'D')\n",
    "cloud_data = cloud_data.drop('Date', axis=1)\n",
    "nan_indexes = cloud_data[cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "good_indexes = cloud_data[~cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "\n",
    "train_cloud = cloud_data.loc[good_indexes]\n",
    "test_cloud = cloud_data.loc[nan_indexes]\n",
    "print(train_cloud.shape)\n",
    "print(test_cloud.shape)\n",
    "\n",
    "X_train_cloud = train_cloud.copy()\n",
    "X_train_cloud = X_train_cloud.drop(['CloudCover'], axis=1)\n",
    "y_train_cloud = train_cloud['CloudCover']\n",
    "test_cloud = test_cloud.drop('CloudCover',axis=1)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_cloud = scaler.fit_transform(X_train_cloud)\n",
    "test_cloud = scaler.transform(test_cloud)\n",
    "\n",
    "if(cv_mode):\n",
    "    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(X_train_cloud)\n",
    "    score_CV = cross_val_score(rf, X_train_cloud, y_train_cloud, cv=kf)\n",
    "\n",
    "    print(\"The 10-fold crossvalidation of RF is {:.5f} +/- {:.3f}\".format(score_CV.mean(),score_CV.std()))\n",
    "    os.system('say \"Crossvalidation done.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "100%|██████████| 49599/49599 [01:29<00:00, 551.53it/s]\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train_cloud,y_train_cloud)\n",
    "pred_results = rf.predict(test_cloud)\n",
    "nan_index = cloud_data[cloud_data['CloudCover'].isnull()].index.tolist()\n",
    "result_df = pd.DataFrame(data={'Index' : nan_index,\n",
    "                               'CloudCover': pred_results}).set_index('Index')\n",
    "#replace missing values\n",
    "for i in tqdm(nan_index):\n",
    "    all_data.loc[i,'CloudCover'] = result_df.loc[i,'CloudCover']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing day\n",
    "Missing day 2016-03-03 in the whole in dataset. It has been decided to ignore it since it wouldn't affect much the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[all_data.Date == '2016-03-03'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region 2 missing values\n",
    "Fill missing data in datetime range 2017-07-04/2018-01-03 (extremes included)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisys of missing dates window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>Events</th>\n",
       "      <th>Max_Dew_PointC</th>\n",
       "      <th>Max_Humidity</th>\n",
       "      <th>Max_Sea_Level_PressurehPa</th>\n",
       "      <th>Max_TemperatureC</th>\n",
       "      <th>Max_VisibilityKm</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Dew_PointC</th>\n",
       "      <th>Mean_Humidity</th>\n",
       "      <th>Mean_Sea_Level_PressurehPa</th>\n",
       "      <th>Mean_TemperatureC</th>\n",
       "      <th>Mean_VisibilityKm</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>Min_Dew_PointC</th>\n",
       "      <th>Min_Humidity</th>\n",
       "      <th>Min_Sea_Level_PressurehPa</th>\n",
       "      <th>Min_TemperatureC</th>\n",
       "      <th>Min_VisibilitykM</th>\n",
       "      <th>Precipitationmm</th>\n",
       "      <th>WindDirDegrees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>1004</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>11805</td>\n",
       "      <td>2</td>\n",
       "      <td>32221</td>\n",
       "      <td>16186</td>\n",
       "      <td>5727</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>15</td>\n",
       "      <td>94</td>\n",
       "      <td>1013</td>\n",
       "      <td>17</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>92</td>\n",
       "      <td>1012</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>1010</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>1004</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>11805</td>\n",
       "      <td>2</td>\n",
       "      <td>32221</td>\n",
       "      <td>16186</td>\n",
       "      <td>5727</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Rain-Thunderstorm</td>\n",
       "      <td>13</td>\n",
       "      <td>94</td>\n",
       "      <td>1019</td>\n",
       "      <td>17</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>1015</td>\n",
       "      <td>13</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>1013</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>1004</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>11805</td>\n",
       "      <td>2</td>\n",
       "      <td>32221</td>\n",
       "      <td>16186</td>\n",
       "      <td>5727</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fog</td>\n",
       "      <td>-2</td>\n",
       "      <td>100</td>\n",
       "      <td>1036</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>-5</td>\n",
       "      <td>95</td>\n",
       "      <td>1035</td>\n",
       "      <td>-5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-10</td>\n",
       "      <td>80</td>\n",
       "      <td>1034</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>1004</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>11805</td>\n",
       "      <td>2</td>\n",
       "      <td>32221</td>\n",
       "      <td>16186</td>\n",
       "      <td>5727</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Fog-Rain</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1034</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35</td>\n",
       "      <td>-3</td>\n",
       "      <td>96</td>\n",
       "      <td>1031</td>\n",
       "      <td>-4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>-12</td>\n",
       "      <td>87</td>\n",
       "      <td>1029</td>\n",
       "      <td>-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      StoreID       Date  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "3403     1004 2017-07-02      False   False          False  Hyper Market   \n",
       "3404     1004 2017-07-03      False    True           True  Hyper Market   \n",
       "3405     1004 2018-01-04       True   False          False  Hyper Market   \n",
       "3406     1004 2018-01-05      False    True          False  Hyper Market   \n",
       "\n",
       "     AssortmentType  NearestCompetitor Region  Region_AreaKM2  Region_GDP  \\\n",
       "3403        General              11805      2           32221       16186   \n",
       "3404        General              11805      2           32221       16186   \n",
       "3405        General              11805      2           32221       16186   \n",
       "3406        General              11805      2           32221       16186   \n",
       "\n",
       "      Region_PopulationK  CloudCover             Events  Max_Dew_PointC  \\\n",
       "3403                5727         7.0               Rain              15   \n",
       "3404                5727         5.0  Rain-Thunderstorm              13   \n",
       "3405                5727         5.0                Fog              -2   \n",
       "3406                5727         7.0           Fog-Rain               2   \n",
       "\n",
       "      Max_Humidity  Max_Sea_Level_PressurehPa  Max_TemperatureC  \\\n",
       "3403            94                       1013                17   \n",
       "3404            94                       1019                17   \n",
       "3405           100                       1036                 0   \n",
       "3406           100                       1034                 3   \n",
       "\n",
       "      Max_VisibilityKm  Max_Wind_SpeedKm_h  Mean_Dew_PointC  Mean_Humidity  \\\n",
       "3403              10.0                  27               13             92   \n",
       "3404              10.0                  26               11             85   \n",
       "3405               6.0                  11               -5             95   \n",
       "3406              10.0                  35               -3             96   \n",
       "\n",
       "      Mean_Sea_Level_PressurehPa  Mean_TemperatureC  Mean_VisibilityKm  \\\n",
       "3403                        1012                 14                9.0   \n",
       "3404                        1015                 13               10.0   \n",
       "3405                        1035                 -5                2.0   \n",
       "3406                        1031                 -4                6.0   \n",
       "\n",
       "      Mean_Wind_SpeedKm_h  Min_Dew_PointC  Min_Humidity  \\\n",
       "3403                   14              11            88   \n",
       "3404                   14               9            63   \n",
       "3405                    6             -10            80   \n",
       "3406                   11             -12            87   \n",
       "\n",
       "      Min_Sea_Level_PressurehPa  Min_TemperatureC  Min_VisibilitykM  \\\n",
       "3403                       1010                12               5.0   \n",
       "3404                       1013                11               4.0   \n",
       "3405                       1034               -10               0.0   \n",
       "3406                       1029               -11               1.0   \n",
       "\n",
       "      Precipitationmm  WindDirDegrees  \n",
       "3403              0.0             217  \n",
       "3404              0.0             260  \n",
       "3405              0.0             232  \n",
       "3406              0.0             243  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing window data from store 1004 from Region 2\n",
    "all_data[all_data.StoreID == 1004].loc[(all_data.Date >= '2017-07-02') & (all_data.Date <= '2018-01-05')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset of a store:\n",
      "- from region 2: 606\n",
      "- from another region: 790\n",
      "\n",
      "total missing: 184\n"
     ]
    }
   ],
   "source": [
    "# Compare size of the dataset with a store without missing window date\n",
    "print('''Size of the dataset of a store:\n",
    "- from region 2: {}\n",
    "- from another region: {}\n",
    "\n",
    "total missing: {}'''\n",
    "      .format(all_data[all_data.StoreID == 1004].shape[0],\n",
    "              all_data[all_data.StoreID == 1006].shape[0],\n",
    "             all_data[all_data.StoreID == 1006].shape[0]-all_data[all_data.StoreID == 1004].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window dates filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Region\n",
       "0 2017-07-04      2\n",
       "1 2017-07-05      2\n",
       "2 2017-07-06      2\n",
       "3 2017-07-07      2\n",
       "4 2017-07-08      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create date range dataframe\n",
    "date_missing_start = '2017-07-04'\n",
    "date_missing_end = '2018-01-03'\n",
    "\n",
    "date_range = pd.date_range(date_missing_start, date_missing_end)\n",
    "\n",
    "# create df and set dtypes\n",
    "df_region2 = pd.DataFrame(data={'Date': date_range, 'Region': '2'})\n",
    "\n",
    "df_region2.Date = pd.to_datetime(df_region2.Date)\n",
    "df_region2.Region = df_region2.Region.astype('category')\n",
    "\n",
    "print(df_region2.shape)\n",
    "df_region2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacement by similar store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HasPromotions (replacement with store 1006 since all are equal)\n",
    "df_region2['HasPromotions'] = all_data[all_data.StoreID == 1006]\\\n",
    ".loc[(all_data.Date >= date_missing_start) & (all_data.Date <= date_missing_end)]\\\n",
    ".HasPromotions.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacement by most correlated region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all_data by Region\n",
    "dic_all = {\n",
    "    'IsHoliday': 'mean',\n",
    "    'IsOpen': 'mean',\n",
    "    'HasPromotions': 'mean',\n",
    "    'NearestCompetitor': 'mean',\n",
    "    'Region_AreaKM2': 'mean',\n",
    "    'Region_GDP': 'mean',\n",
    "    'Region_PopulationK': 'mean',\n",
    "    'CloudCover': 'mean',\n",
    "    'Max_Humidity': 'mean',\n",
    "    'Max_Dew_PointC': 'mean',\n",
    "    'Max_Sea_Level_PressurehPa': 'mean',\n",
    "    'Max_TemperatureC': 'mean',\n",
    "    'Max_VisibilityKm': 'mean',\n",
    "    'Max_Wind_SpeedKm_h': 'mean',\n",
    "    'Mean_Dew_PointC': 'mean',\n",
    "    'Mean_Humidity': 'mean',\n",
    "    'Mean_Sea_Level_PressurehPa': 'mean',\n",
    "    'Mean_TemperatureC': 'mean',\n",
    "    'Mean_VisibilityKm': 'mean',\n",
    "    'Mean_Wind_SpeedKm_h': 'mean',\n",
    "    'Min_Dew_PointC': 'mean',\n",
    "    'Min_Humidity': 'mean',\n",
    "    'Min_Sea_Level_PressurehPa': 'mean',\n",
    "    'Min_TemperatureC': 'mean',\n",
    "    'Min_VisibilitykM': 'mean',\n",
    "    'Precipitationmm': 'mean',\n",
    "    'WindDirDegrees': 'mean'}\n",
    "\n",
    "\n",
    "# Define train_data grouped by Region in common date range\n",
    "all_data_byRegion = all_data.groupby(['Region', pd.Grouper(key='Date', freq='D')])\\\n",
    ".agg(dic_all).reset_index()\n",
    "\n",
    "all_data_common_byRegion =\\\n",
    "all_data_byRegion.loc[(all_data_byRegion.Date >= '2016-03-01') &\n",
    "                      (all_data_byRegion.Date <= '2017-07-03')]\n",
    "\n",
    "all_data_missing_byRegion =\\\n",
    "all_data_byRegion[all_data_byRegion.Region != '2']\\\n",
    ".loc[(all_data_byRegion.Date >= date_missing_start) &\n",
    "     (all_data_byRegion.Date <= date_missing_end)]\n",
    "\n",
    "# Define train_data grouped by Region in common date range\n",
    "all_data_byRegion_except_region2 = all_data_common_byRegion[all_data_common_byRegion.Region != '2']\n",
    "all_data_byRegion_except_region2.Region.cat.remove_unused_categories(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated region for the given feature\n",
      "FEATURE                       REGION    CORRELATION\n",
      "--------------------------------------------------\n",
      "Max_TemperatureC              6             0.9603\n",
      "CloudCover                    1             0.5296\n",
      "Max_Sea_Level_PressurehPa     6             0.9825\n",
      "Max_Wind_SpeedKm_h            6             0.6525\n",
      "Max_Dew_PointC                6             0.9609\n",
      "WindDirDegrees                6             0.5041\n",
      "Mean_Dew_PointC               6             0.9597\n",
      "Mean_Sea_Level_PressurehPa    6             0.9892\n",
      "Mean_TemperatureC             6             0.9668\n",
      "Mean_Wind_SpeedKm_h           6             0.7335\n",
      "Min_Dew_PointC                6             0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaprimo/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/numpy/lib/function_base.py:3183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/flaprimo/Development/Workspaces/Python/sales-forecast/venv/lib/python3.6/site-packages/numpy/lib/function_base.py:3184: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_Sea_Level_PressurehPa     6             0.9816\n",
      "Min_VisibilitykM              6             0.3587\n"
     ]
    }
   ],
   "source": [
    "# Replace by most correlated region\n",
    "median_correction = ['Max_TemperatureC', 'CloudCover', 'Max_Sea_Level_PressurehPa',\n",
    "                     'Max_Wind_SpeedKm_h', 'Max_Dew_PointC', 'WindDirDegrees',\n",
    "                     'Mean_Dew_PointC', 'Mean_Sea_Level_PressurehPa', 'Mean_TemperatureC',\n",
    "                     'Mean_Wind_SpeedKm_h', 'Min_Dew_PointC', 'Min_Sea_Level_PressurehPa',\n",
    "                     'Min_VisibilitykM']\n",
    "\n",
    "print('Most correlated region for the given feature')\n",
    "print('{:<30}{:10}{:>10}'.format('FEATURE', 'REGION', 'CORRELATION'))\n",
    "print('-' * (30+10+10))\n",
    "\n",
    "# get the date range\n",
    "for feature_name in median_correction:\n",
    "    # compute correlation for each region wrt region 2 for current feature\n",
    "    corr_byRegion = {}\n",
    "    for region_name, region in all_data_byRegion_except_region2.groupby('Region'):\n",
    "        region = region[feature_name]\n",
    "        region_2 = all_data_common_byRegion[feature_name][all_data_common_byRegion.Region == '2']\n",
    "        \n",
    "        corr_byRegion[region_name] = pd.np.corrcoef(region_2, region)[1,0]\n",
    "    \n",
    "    # get region with max correlation for the current feature\n",
    "    max_corr_region = max(corr_byRegion, key=corr_byRegion.get)\n",
    "    \n",
    "    print('{:<30}{:10}{:10.4f}'.format(feature_name, max_corr_region, corr_byRegion[max_corr_region]))\n",
    "    \n",
    "    # replace missing feature for date window with most correlated region feature \n",
    "    feature_column = all_data_missing_byRegion[all_data_missing_byRegion.Region == max_corr_region]\\\n",
    "    [feature_name].reset_index().drop('index', axis=1)\n",
    "    \n",
    "    df_region2 = pd.concat([df_region2, feature_column], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacement by region 2 itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from Region 2: Region_AreaKM2, Region_GDP, Region_PopulationK\n",
    "for feature_name in ['Region_AreaKM2', 'Region_GDP', 'Region_PopulationK']:\n",
    "    df_region2[feature_name] = all_data[all_data.Region == '2'][feature_name].unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacement by single store from Region 2 and concatenation with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568710, 33)\n",
      "(591710, 33)\n"
     ]
    }
   ],
   "source": [
    "# Add missing dates window to dataset\n",
    "print(all_data.shape)\n",
    "\n",
    "# add to all_data missing region 2 values\n",
    "for storeid in all_data[all_data.Region == '2'].StoreID.unique():    \n",
    "    # copy from same stores from Region 2: StoreID, AssortmentType, NearestCompetitor, StoreType\n",
    "    store_data = {\n",
    "        'StoreID': np.array([storeid], dtype='uint16'),\n",
    "        'AssortmentType': all_data[all_data.StoreID == storeid].AssortmentType.unique()[0],\n",
    "        'NearestCompetitor': all_data[all_data.StoreID == storeid].NearestCompetitor.unique()[0],\n",
    "        'StoreType': all_data[all_data.StoreID == storeid].StoreType.unique()[0]\n",
    "    }\n",
    "    \n",
    "    # create 'storeid' column\n",
    "    df_storeid = pd.DataFrame(data=store_data, index=np.arange(0, df_region2.shape[0]))\n",
    "    \n",
    "    df_storeid = pd.concat([df_region2, df_storeid], axis=1)\n",
    "    \n",
    "    # add to all_data df the missing values range for 'storeid'\n",
    "    all_data = pd.concat([all_data, df_storeid], axis=0, ignore_index=True, sort=True)\n",
    "\n",
    "all_data = all_data\\\n",
    ".sort_values(by=['Region', 'StoreID', 'Date'], ascending=[False, False, True])\\\n",
    ".reset_index().drop('index', axis=1)\n",
    "\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AssortmentType                False\n",
       "CloudCover                    False\n",
       "Date                          False\n",
       "Events                         True\n",
       "HasPromotions                 False\n",
       "IsHoliday                      True\n",
       "IsOpen                         True\n",
       "Max_Dew_PointC                False\n",
       "Max_Humidity                   True\n",
       "Max_Sea_Level_PressurehPa     False\n",
       "Max_TemperatureC              False\n",
       "Max_VisibilityKm               True\n",
       "Max_Wind_SpeedKm_h            False\n",
       "Mean_Dew_PointC               False\n",
       "Mean_Humidity                  True\n",
       "Mean_Sea_Level_PressurehPa    False\n",
       "Mean_TemperatureC             False\n",
       "Mean_VisibilityKm              True\n",
       "Mean_Wind_SpeedKm_h           False\n",
       "Min_Dew_PointC                False\n",
       "Min_Humidity                   True\n",
       "Min_Sea_Level_PressurehPa     False\n",
       "Min_TemperatureC               True\n",
       "Min_VisibilitykM              False\n",
       "NearestCompetitor             False\n",
       "Precipitationmm                True\n",
       "Region                        False\n",
       "Region_AreaKM2                False\n",
       "Region_GDP                    False\n",
       "Region_PopulationK            False\n",
       "StoreID                       False\n",
       "StoreType                     False\n",
       "WindDirDegrees                False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TODO:')\n",
    "all_data[all_data.Region == '2'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store is open and is holiday\n",
    "This feature has been added for those stores that are open during holidays: we want to inspect if the sales are higher in those days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_open_holiday = all_data.IsHoliday & all_data.IsOpen\n",
    "all_data['Hol_and_open'] = is_open_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting reasons\n",
    "holidays_open_mask = train.IsHoliday & train.IsOpen\n",
    "open_mask = train['IsOpen'] == True\n",
    "not_holidays_mask =  train['IsHoliday'] == False\n",
    "not_holidays_open_mask = not_holidays_mask & open_mask\n",
    "\n",
    "df_holidays_open = train[holidays_open_mask].groupby(\"StoreID\")\n",
    "avg_sales_holidays = df_holidays_open.NumberOfSales.mean()\n",
    "avg_sales_holidays.plot(figsize=(16,8),legend=True,marker='o')\n",
    "\n",
    "df_not_holidays_open = train[not_holidays_open_mask].groupby(\"StoreID\")\n",
    "avg_sales_not_holidays = df_not_holidays_open.NumberOfSales.mean()\n",
    "avg_sales_not_holidays = avg_sales_not_holidays.filter(items = avg_sales_holidays.index)\n",
    "avg_sales_not_holidays.plot(figsize=(16,8),legend=True,marker='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region's population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Region_PD'] = all_data.Region_PopulationK.div(all_data.Region_AreaKM2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PD = all_data.groupby(\"Region\").Region_PD.mean().sort_values(ascending=False)\\\n",
    "        .plot(kind='bar', title='Population Density', figsize=(16,8));\n",
    "plot_PD.set_ylabel(\"K people / squared Km\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain weeks\n",
    "weeks = all_data[['Date']].groupby(pd.Grouper(key='Date', freq='W')).sum().reset_index()\n",
    "\n",
    "all_data['week_of_month'] = (weeks.Date.dt.day - 1) // 7 + 1 # number of the week in the month\n",
    "\n",
    "all_data['year'] = all_data.Date.dt.year\n",
    "all_data['month'] = all_data.Date.dt.month\n",
    "all_data['day_of_month'] = all_data.Date.dt.day\n",
    "all_data['day_of_week'] = all_data.Date.dt.dayofweek\n",
    "all_data['day_of_year'] = all_data.Date.dt.dayofyear\n",
    "all_data['WeekOfYear'] = all_data.Date.dt.weekofyear\n",
    "all_data['days_in_month'] = all_data.Date.dt.daysinmonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data,columns=['Region','AssortmentType', 'StoreType', 'Events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_features(regional_week_year, week, year, column_of_interest):\n",
    "    num_holidays = 0\n",
    "\n",
    "    if week == 1:\n",
    "        week_before = [52,year-1]\n",
    "        week_after = [2,year]\n",
    "    if week == 52:\n",
    "        week_after = [1,year+1]\n",
    "        week_before = [51,year]\n",
    "    if (week>1) & (week<52):\n",
    "        week_before = [week-1,year]\n",
    "        week_after = [week+1,year]\n",
    "    \n",
    "    before_df = regional_week_year[(regional_week_year['WeekOfYear']==week_before[0])&\n",
    "                                  (regional_week_year['year']==week_before[1])]\n",
    "    \n",
    "    after_df = regional_week_year[(regional_week_year['WeekOfYear']==week_after[0])&\n",
    "                                  (regional_week_year['year']==week_after[1])]\n",
    "    this_df = regional_week_year[(regional_week_year['WeekOfYear']==week)&\n",
    "                                 (regional_week_year['year']==year)]\n",
    "    \n",
    "    num_ago = before_df[before_df[column_of_interest]][column_of_interest].sum()\n",
    "    num_next = after_df[after_df[column_of_interest]][column_of_interest].sum()\n",
    "    num_this = this_df[this_df[column_of_interest]][column_of_interest].sum()\n",
    "    \n",
    "    return num_ago, num_next, num_this\n",
    "\n",
    "def create_new_features(before,current,after,column_of_interest):\n",
    "    all_data[before] = 0\n",
    "    all_data[current] = 0\n",
    "    all_data[after] = 0\n",
    "\n",
    "    region_list= [\"Region_\"+str(d) for d in range(0,11)]\n",
    "    for region in tqdm(region_list):\n",
    "        curr_region = all_data[all_data[region]==1]\n",
    "\n",
    "        #get all valid dates of that region\n",
    "        regional_week_year=curr_region[['WeekOfYear','year',column_of_interest]]\n",
    "\n",
    "        #get all store ids of that region\n",
    "        regional_stores=len(curr_region['StoreID'].unique())\n",
    "\n",
    "        week_year_list =regional_week_year[['WeekOfYear','year']].drop_duplicates().values.tolist()\n",
    "\n",
    "        for date in week_year_list:\n",
    "            num_ago,num_next,num_this = num_of_features(regional_week_year,\n",
    "                                                        date[0],\n",
    "                                                        date[1],\n",
    "                                                        column_of_interest)\n",
    "\n",
    "            all_data.at[((all_data['WeekOfYear']==date[0])&\n",
    "                         (all_data['year']==date[1])),before]=int(num_ago/regional_stores)\n",
    "            all_data.at[((all_data['WeekOfYear']==date[0])&\n",
    "                         (all_data['year']==date[1])),after]=int(num_next/regional_stores)\n",
    "            all_data.at[((all_data['WeekOfYear']==date[0])&\n",
    "                         (all_data['year']==date[1])),current]=int(num_this/regional_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_features('HolidaysWeekBefore', 'HolidaysWeekCurrent', 'HolidaysWeekAfter','IsHoliday')\n",
    "create_new_features('PromoWeekBefore', 'PromoWeekCurrent', 'PromoWeekAfter', 'HasPromotions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = all_data[:train.shape[0]].copy()\n",
    "train_final['NumberOfSales'] = train.NumberOfSales\n",
    "train_final['NumberOfCustomers'] = train.NumberOfSales\n",
    "\n",
    "# extract the test examples (we don't have the class value for this)\n",
    "test_final = all_data[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_2016  = datetime.date(2016,3,31)\n",
    "monthly_sales['Differential'] = 0        \n",
    "        \n",
    "for storeID, store in tqdm(monthly_sales.groupby('StoreID')):\n",
    "    monthly_sales.loc[store.index,'one_step'] = store['NumberOfSales'].shift(1).fillna(0)\n",
    "    monthly_sales.loc[store.index,'two_steps'] = store['NumberOfSales'].shift(2).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final['Differential'] = 0\n",
    "\n",
    "#correct one step in april\n",
    "april_mask = monthly_sales.Date.dt.month == 4\n",
    "year_mask = monthly_sales.Date.dt.year == 2016\n",
    "monthly_sales.loc[year_mask & april_mask,'one_step'] = 0\n",
    "\n",
    "#compute differential\n",
    "monthly_sales['Differential'] = monthly_sales['one_step'] - monthly_sales['two_steps']\n",
    "pippo = monthly_sales[['StoreID','Date','Differential']]\n",
    "\n",
    "#longest for ever - more than 10 mins - there must be a better way..\n",
    "for index,row in tqdm(pippo.iterrows()):\n",
    "    differential = row.Differential\n",
    "    month = row.Date.month\n",
    "    year = row.Date.year\n",
    "    store = row.StoreID\n",
    "    month_mask = train_final.Date.dt.month == month\n",
    "    year_mask = train_final.Date.dt.year == year\n",
    "    store_mask = train_final.StoreID == store\n",
    "    \n",
    "    train_final.loc[store_mask & year_mask & month_mask,'Differential'] = differential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final.to_csv(data_path_out+'train_final.csv')\n",
    "# test_final.to_csv(data_path_out+'test_final.csv')\n",
    "\n",
    "# Serialize dataframes for later use in implementations\n",
    "with open(data_path_out+'train_pp.obj', 'wb') as file:\n",
    "        pickle.dump(train_final, file)\n",
    "# with open(data_path_in+'test_pp.obj', 'wb') as file:\n",
    "#         pickle.dump(test_clean, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
